{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minuting_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAAcv8V5ezfQ"
      },
      "source": [
        "## Extracting test data from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vNI8UL-bdYT"
      },
      "source": [
        "import os "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sWmS4zNeO5h"
      },
      "source": [
        "root_dir = '/content/drive/MyDrive/automin-2021-confindential-data-main/task-A-elitr-minuting-corpus-en'\n",
        "test_file_name = 'test.json'\n",
        "test_data = os.path.join(root_dir, test_file_name)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ccBxG_tvanU"
      },
      "source": [
        "import json\n",
        "\n",
        "filehandle = open(test_data, 'r')\n",
        "\n",
        "test_file = json.load(filehandle)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Fk3zHFvajq",
        "outputId": "88ef54e1-bb7f-4861-b648-ebf0ed9a4ca6"
      },
      "source": [
        "test_file.keys()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['meeting_en_test_009', 'meeting_en_test_028', 'meeting_en_test_027', 'meeting_en_test_026', 'meeting_en_test_025', 'meeting_en_test_024', 'meeting_en_test_023', 'meeting_en_test_022', 'meeting_en_test_021', 'meeting_en_test_020', 'meeting_en_test_019', 'meeting_en_test_017', 'meeting_en_test_004', 'meeting_en_test_016', 'meeting_en_test_012', 'meeting_en_test_005', 'meeting_en_test_013', 'meeting_en_test_015', 'meeting_en_test_008', 'meeting_en_test_006', 'meeting_en_test_002', 'meeting_en_test_010', 'meeting_en_test_018', 'meeting_en_test_011', 'meeting_en_test_003', 'meeting_en_test_014', 'meeting_en_test_001', 'meeting_en_test_007'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz-6qggbvaQb"
      },
      "source": [
        "def zip_documents(roles, utterances):\n",
        "  sentences = zip(roles, utterances)\n",
        "  extraction = [f'{role}: {utterance}' for role, utterance in sentences]\n",
        "  return extraction"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcF4JFMVx_RD"
      },
      "source": [
        "skipped_files = []\n",
        "processed_files = dict()\n",
        "length_dict = dict()\n",
        "for document in test_file.keys():\n",
        "  roles = test_file[document]['roles']\n",
        "  utterances = test_file[document]['utterances']\n",
        "  if len(roles) != len(utterances):\n",
        "    skipped_files.append(document)\n",
        "    continue\n",
        "  processed_files[document] = dict()\n",
        "  processed_files[document]['transcript'] = zip_documents(roles, utterances)\n",
        "  length_dict[document]= len(processed_files[document]['transcript'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkulS_4kmR_f"
      },
      "source": [
        "def doc_partitioning(document, max_characters=1700):\n",
        "  \n",
        "  processed_dict = dict()\n",
        "  processed_dict['part_0'] = ''\n",
        "  identity_generator = 'part_'\n",
        "  temp = ''\n",
        "  count = 0\n",
        "  for sentence in document:\n",
        "    key = f'{identity_generator}{count}'\n",
        "    temp = temp + sentence\n",
        "    if len(temp) > max_characters:\n",
        "      temp = ''\n",
        "      count = count + 1\n",
        "      key = f'{identity_generator}{count}'\n",
        "      processed_dict[key] = ''\n",
        "    processed_dict[key] = processed_dict[key] +'\\n'+  sentence\n",
        "\n",
        "  return processed_dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp8FCIj8qS7k",
        "outputId": "6716b781-e706-4d29-d60e-5252115dbbec"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nbpRlkEpbik"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"lidiya/bart-large-xsum-samsum\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD_hNO3utdxr"
      },
      "source": [
        "def apply_summarizer(processed_dict):\n",
        "  Output = []\n",
        "  for key in processed_dict.keys():\n",
        "    result = summarizer(processed_dict[key])\n",
        "    Output.append(result[0]['summary_text'])\n",
        "\n",
        "  return Output\n",
        "\n",
        "def split_Sentences(summarizer_output):\n",
        "  \n",
        "  Result = []\n",
        "\n",
        "  for text in summarizer_output:\n",
        "    sentences = text.split('.')\n",
        "    for sentence in sentences:\n",
        "      sentence = sentence.strip()\n",
        "      if sentence != '':\n",
        "        Result.append(sentence)\n",
        "\n",
        "  return Result"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjPYW4ff0q_x"
      },
      "source": [
        "import datetime \n",
        "\n",
        "def return_date():\n",
        "\n",
        "  Date = f'Date: {datetime.datetime.now().strftime(\"%Y-%m-%d\")}'\n",
        "\n",
        "  return Date"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpJ0ffb31j9Y"
      },
      "source": [
        "def main_body(output):\n",
        "\n",
        "  body = []\n",
        "\n",
        "  body_header = 'SUMMARY- \\n'\n",
        "  for sentence in output:\n",
        "    \n",
        "    if len(sentence.split(' ')) > 3:\n",
        "      body.append(f'-{sentence}')\n",
        "    \n",
        "  body_content = '\\n'.join(body)  \n",
        "  body_Block = f'{body_header}{body_content}'\n",
        "  \n",
        "  return body_Block"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogbZRtbI4W8S"
      },
      "source": [
        "import re\n",
        "\n",
        "def generate_person_list(Output):\n",
        "\n",
        "  reg_ex = [r'PERSON[0-9]{1,2}']\n",
        "  person_list = []\n",
        "\n",
        "  for sentence in Output:\n",
        "    result = re.search(reg_ex[0], sentence)\n",
        "    try:\n",
        "      person_list.append(result.group())\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return list(set(person_list))\n",
        "\n",
        "def generate_attendes(person_list):\n",
        "\n",
        "  attendee_header = 'ATTENDEES: '\n",
        "  attendee_content = attendee_header + ', '.join(person_list)\n",
        "\n",
        "  return attendee_content"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTTg937svLe6"
      },
      "source": [
        "def prepare_document(attendee_str, body, annotator = 'Nidhir'):\n",
        "\n",
        "  Date_ = return_date()\n",
        "  Document = f'{Date_}\\n{attendee_str}\\n\\n\\n{body}\\n\\nMinuted by: {annotator}'\n",
        "\n",
        "  return Document"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnxeIYmP6Q3-",
        "outputId": "41766c26-c9dd-4004-c14d-267c24584bbf"
      },
      "source": [
        "cd /content/drive/MyDrive/task_A_test_results"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/task_A_test_results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7VC0-1M6m7t",
        "outputId": "80cc5dd7-9a8f-45a3-ff7b-569904fdc3d8"
      },
      "source": [
        "# main pipeline\n",
        "\n",
        "# Allocate transcript\n",
        "key = 'meeting_en_test_025'\n",
        "transcript = processed_files[key]['transcript']\n",
        "\n",
        "# document partitioning\n",
        "processed_dict = doc_partitioning(transcript, max_characters=2000)\n",
        "\n",
        "# apply summarizer to processed transcript\n",
        "Output = apply_summarizer(processed_dict)\n",
        "Output = split_Sentences(Output)\n",
        "\n",
        "# Minute parts\n",
        "person_List = generate_person_list(Output)\n",
        "Attendees = generate_attendes(person_List) \n",
        "Main_body = main_body(Output)\n",
        "\n",
        "# Assemle parts and preparing final minute:\n",
        "DOCUMENT = prepare_document(Attendees, Main_body)\n",
        "print(DOCUMENT)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Date: 2021-07-24\n",
            "ATTENDEES: PERSON7, PERSON4, PERSON5, PERSON2, PERSON12, PERSON1, PERSON3, PERSON6\n",
            "\n",
            "\n",
            "SUMMARY- \n",
            "-PERSON1 was in a call until the very last minute and his machine crashed, so he had to restart it\n",
            "-Now he can hear people again\n",
            "-His mother is writing the question for the German subti subti subtitle user study and she has some progress\n",
            "-PERSON4 started to write the deliverables and asked PERSON5 to do the transcripts of by German ASR f for the German transcriber\n",
            "-PERSON5 has just added the transcripts for the first 30 parts and for the next ten parts, but for some folders the audio format is wrong\n",
            "-PERSON1 and PERSON5 want to implement natural shortening in their models\n",
            "-PROJECT4 is not ready for speech translation\n",
            "-PERSON1, PERSON5, PERSON2, PERSON6 and PERSON3 are looking for students for the PROJECT4 project\n",
            "-PERSON12 works halfway in Prague and halfway in Brno, but he is working on other things\n",
            "-PERSON1, PERSON6 and PERSON3 will meet tomorrow in the afternoon to have a dry run of some past similar talks to (PROJECT2) (docs) and see how the system works\n",
            "-PERSON1, PERSON3 and PERSON6 are going to have a test on Friday\n",
            "-They will have two separate pipelines for two concurrent sessions\n",
            "-PERSON3 is compiling some clusters for PROJECT1 test set\n",
            "-Some of the files are not connected properly into a 16-cave format, so he has to convert them manually\n",
            "-The sound quality is a little worse when passed to the A S I sound system\n",
            "-PERSON1\n",
            "-PERSON3 will send the numbers for PROJECT1 test set to PERSON1 today\n",
            "-PERSON1, PERSON3 and PERSON6 are going to do the PROJECT1 test for the two domains tomorrow\n",
            "-They are also going to have a discussion about the dictionaries in the ASR system\n",
            "-PERSON1 recorded the phonemes and added them to his dictionary\n",
            "-The performance of dix (description) was the same with and without adding the pronunciations in their dictionaries\n",
            "-There is a lot of noise in the dictionary\n",
            "-PERSON1 wants PERSON6 and PERSON2 to help him with creating a dictionary\n",
            "-PERSON2 is finishing training of German ASR that may be used for time stamping\n",
            "-There are some serious errors in the ASR output\n",
            "-PERSON4 and PERSON3 need to prepare the Transcripts for (defen) test set and reco recognition for around a 100 hours\n",
            "-PERSON7 has some presentations to make and assignments to hand in\n",
            "-PERSON1 needs to prepare numbers for the different models\n",
            "-PERSON3 needs to\n",
            "-PERSON1 wants the ASR numbers today and the improved numbers on Monday\n",
            "-On Monday, PERSON6 is going to do more of an adaptation for both the different tasks\n",
            "-On the other hand, during the month of August, the rainbow workers had better scores than the other empty models\n",
            "-PERSON5 is trying to commit the file but the output is terrible for some folders\n",
            "-The transcripts for some of the files were processed incorrectly so they don't have the right length\n",
            "-PERSON5 is working on the German ASR\n",
            "-There is a problem with the length of some of the files that she transcribes\n",
            "-She will change the conversing to SOX\n",
            "-She is saving the files so she can run the interpreter in parallel\n",
            "-PERSON2 and PERSON4 are working on a speech\n",
            "-Some of the files do not have punctuation\n",
            "-In some of them, you can hear both the interpreter and the speaker in English\n",
            "-If it doesn't work, PERSON2 will ask the annotator if the transcription\n",
            "-PERSON6 is having problems with the pronunciation of some words in his native language\n",
            "-Person2 sends him a link to a package of the language, but some of the phonemes are not in the alphabet\n",
            "-Person6 is afraid that when processing the list of graphemes to\n",
            "-PERSON2 and PERSON6 are trying to create variable pronunciations\n",
            "-They need to substitute each phoneme with a different one in order to do that\n",
            "-PERSON2 and PERSON6 are going to have a call tomorrow\n",
            "-PERSON2 will send a translation of a dictionary to (CMU) and then they will have a test run with PERSON3\n",
            "-PERSON6 wants PERSON2 to convert the list into the CMU alphabet phonemes\n",
            "-Person2 will do it tomorrow\n",
            "\n",
            "Minuted by: Nidhir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekAYgxOtcALy"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}